{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"},{"sourceId":8609944,"sourceType":"datasetVersion","datasetId":5152307}],"dockerImageVersionId":30732,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-05T05:34:03.967216Z","iopub.execute_input":"2024-06-05T05:34:03.968024Z","iopub.status.idle":"2024-06-05T05:34:04.988343Z","shell.execute_reply.started":"2024-06-05T05:34:03.967986Z","shell.execute_reply":"2024-06-05T05:34:04.987409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install sentencepiece","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch \nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModel \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport os \nimport gc \nimport re \nimport torch.nn.functional as F\nfrom tqdm import tqdm\nimport string\nfrom sklearn.model_selection import train_test_split\nimport torch.nn as nn\nimport torch.optim as optim","metadata":{"execution":{"iopub.status.busy":"2024-06-05T05:34:11.620109Z","iopub.execute_input":"2024-06-05T05:34:11.621138Z","iopub.status.idle":"2024-06-05T05:34:16.431895Z","shell.execute_reply.started":"2024-06-05T05:34:11.621098Z","shell.execute_reply":"2024-06-05T05:34:16.431038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading the training dataset from a CSV file into a Pandas DataFrame\ntrain = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv\")\nprint(train.head(5))\nprint(train.shape)","metadata":{"execution":{"iopub.status.busy":"2024-06-05T05:34:16.433329Z","iopub.execute_input":"2024-06-05T05:34:16.433750Z","iopub.status.idle":"2024-06-05T05:34:17.148177Z","shell.execute_reply.started":"2024-06-05T05:34:16.433725Z","shell.execute_reply":"2024-06-05T05:34:17.147245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv\")\nprint(test.head(5))\nprint(test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-06-05T05:34:17.149245Z","iopub.execute_input":"2024-06-05T05:34:17.149540Z","iopub.status.idle":"2024-06-05T05:34:17.512895Z","shell.execute_reply.started":"2024-06-05T05:34:17.149516Z","shell.execute_reply":"2024-06-05T05:34:17.511610Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_classes = 6\nplt.figure(figsize=(10, 6))\nplt.hist(train['score'], bins=num_classes, edgecolor='black')\nplt.title('Distribution of Scores')\nplt.xlabel('Score')\nplt.ylabel('Frequency')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-05T05:34:17.515504Z","iopub.execute_input":"2024-06-05T05:34:17.515806Z","iopub.status.idle":"2024-06-05T05:34:17.824533Z","shell.execute_reply.started":"2024-06-05T05:34:17.515779Z","shell.execute_reply":"2024-06-05T05:34:17.823670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfolds = 16\ntrain[\"fold\"] = -1\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=1000)\nfor fold, (train_index, val_index) in enumerate(skf.split(train, train[\"score\"])):\n    train.loc[val_index, \"fold\"] = fold","metadata":{"execution":{"iopub.status.busy":"2024-06-05T05:34:17.825565Z","iopub.execute_input":"2024-06-05T05:34:17.825825Z","iopub.status.idle":"2024-06-05T05:34:17.848709Z","shell.execute_reply.started":"2024-06-05T05:34:17.825802Z","shell.execute_reply":"2024-06-05T05:34:17.847944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_len = 0 \nmax_indx = 0\ntemp = 0\nfor text in train.full_text.values: \n    text = text.split() \n    if len(text) > max_len:\n        max_len = len(text)\n        max_indx = temp \n    temp += 1 \nprint(f\"Max length by whitespace splitting: {max_len}\") \nprint(\"-------------LARGEST ESSAY-------------------\")\nprint(train.full_text.values[max_indx])","metadata":{"execution":{"iopub.status.busy":"2024-06-05T05:34:17.849949Z","iopub.execute_input":"2024-06-05T05:34:17.850250Z","iopub.status.idle":"2024-06-05T05:34:18.262089Z","shell.execute_reply.started":"2024-06-05T05:34:17.850223Z","shell.execute_reply":"2024-06-05T05:34:18.261213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process(text):\n    text = text.replace('\\n', ' ').replace('\\t', ' ')\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text = re.sub(' +', ' ', text)\n    text = text.strip()\n    return text\ntrain['full_text'] = train['full_text'].apply(process)","metadata":{"execution":{"iopub.status.busy":"2024-06-05T05:34:18.263240Z","iopub.execute_input":"2024-06-05T05:34:18.263598Z","iopub.status.idle":"2024-06-05T05:34:21.315453Z","shell.execute_reply.started":"2024-06-05T05:34:18.263562Z","shell.execute_reply":"2024-06-05T05:34:21.314612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EmbedDataset(torch.utils.data.Dataset):\n    def __init__(self, data, tokenizer):\n        self.data = data.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.max_len = 0 \n        for i in range(self.data.shape[0]):\n            text = self.data.loc[i, \"full_text\"]\n            tokens = tokenizer(text, add_special_tokens=True)\n            tokens = text.split() \n            self.max_len = max(len(tokens), self.max_len)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        text = self.data.loc[idx, \"full_text\"]\n        tokens = self.tokenizer(text, None, add_special_tokens=True, padding='max_length', truncation=True, max_length=self.max_len, return_tensors=\"pt\")\n        tokens = {k: v.squeeze(0) for k, v in tokens.items()}\n        return tokens","metadata":{"execution":{"iopub.status.busy":"2024-06-05T05:34:21.316516Z","iopub.execute_input":"2024-06-05T05:34:21.316787Z","iopub.status.idle":"2024-06-05T05:34:21.324958Z","shell.execute_reply.started":"2024-06-05T05:34:21.316764Z","shell.execute_reply":"2024-06-05T05:34:21.324116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def word_to_sentence_embeddings(preds, attention):\n    out = preds.last_hidden_state.detach().cpu()\n    padding_mask = attention.unsqueeze(-1).expand(out.size()).float() \n    pool = torch.sum(out * padding_mask, 1)/torch.clamp(padding_mask.sum(1), min=1e-9)\n    return pool ","metadata":{"execution":{"iopub.status.busy":"2024-06-05T05:34:21.327886Z","iopub.execute_input":"2024-06-05T05:34:21.328456Z","iopub.status.idle":"2024-06-05T05:34:21.347851Z","shell.execute_reply.started":"2024-06-05T05:34:21.328416Z","shell.execute_reply":"2024-06-05T05:34:21.347130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_embeddings(model, dataloader): \n    device = 'cuda:0' \n    train_sentence_embeddings = [] \n    for batch in tqdm(dataloader, \"computing word embeddings\"): \n        real_input = batch['input_ids'].to(device)\n        attention_masks = batch['attention_mask'].to(device) \n                \n        with torch.no_grad(): #reduces memory usage by not computing gradients at all \n            with torch.cuda.amp.autocast(): #mixed precision\n                preds = model(input_ids=real_input, attention_mask=attention_masks)\n        pool = word_to_sentence_embeddings(preds, attention_masks.detach().cpu()) \n        sentence_embeddings = F.normalize(pool, p=2, dim=1)\n        sentence_embeddings = sentence_embeddings.squeeze(0).detach().cpu().numpy()\n        train_sentence_embeddings.extend(sentence_embeddings)\n        \n    train_sentence_embeddings = np.array(train_sentence_embeddings)\n    \n    del preds, sentence_embeddings, real_input, attention_masks\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return train_sentence_embeddings","metadata":{"execution":{"iopub.status.busy":"2024-06-05T05:34:21.348912Z","iopub.execute_input":"2024-06-05T05:34:21.349178Z","iopub.status.idle":"2024-06-05T05:34:21.356846Z","shell.execute_reply.started":"2024-06-05T05:34:21.349155Z","shell.execute_reply":"2024-06-05T05:34:21.356135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sentence_embeddings = np.load('/kaggle/input/embeddings-for-aes/train_sentence_embeddings.npy')\ntest_sentence_embeddings = np.load('/kaggle/input/embeddings-for-aes/test_sentence_embeddings.npy')","metadata":{"execution":{"iopub.status.busy":"2024-06-05T06:21:30.669986Z","iopub.execute_input":"2024-06-05T06:21:30.670324Z","iopub.status.idle":"2024-06-05T06:21:30.702429Z","shell.execute_reply.started":"2024-06-05T06:21:30.670298Z","shell.execute_reply":"2024-06-05T06:21:30.701634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_train_embeds = np.concatenate([train_sentence_embeddings], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-06-05T06:24:16.667062Z","iopub.execute_input":"2024-06-05T06:24:16.667442Z","iopub.status.idle":"2024-06-05T06:24:16.698223Z","shell.execute_reply.started":"2024-06-05T06:24:16.667410Z","shell.execute_reply":"2024-06-05T06:24:16.697490Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import cohen_kappa_score\n\ndef comp_score(y_true, y_pred):\n    m = cohen_kappa_score(y_true, y_pred, weights='quadratic')\n    return m\n\nclass AESModel(nn.Module):\n    def __init__(self, input_size, hidden_sizes): \n        '''\n        hidden_sizes is an array of integers\n        This is a multilayer perceptron taking the sentence embeddings from the previous layer, the sentence embeddings is simply an average over the word embeddings of the Deberta model\n        '''\n        super(AESModel, self).__init__()\n        self.input_size = input_size \n        self.hidden_sizes = hidden_sizes\n        \n        self.dense1 = nn.Linear(input_size, hidden_sizes[0])\n        self.activation = nn.ReLU() \n        self.dense2 = nn.Linear(hidden_sizes[0], hidden_sizes[1]) \n        self.dense3 = nn.Linear(hidden_sizes[1], hidden_sizes[2])\n        self.dense4 = nn.Linear(hidden_sizes[2], hidden_sizes[3])\n        self.dense5 = nn.Linear(hidden_sizes[3], hidden_sizes[4])\n        self.dense6 = nn.Linear(hidden_sizes[4], 6) \n        \n    def forward(self, x): \n        x = self.dense1(x)\n        x = self.activation(x)\n        x = self.dense2(x) \n        x = self.activation(x)\n        x = self.dense3(x)\n        x = self.activation(x)\n        x = self.dense4(x) \n        x = self.activation(x)\n        x = self.dense5(x) \n        x = self.activation(x)\n        x = self.dense6(x) \n        return x \n\ndef train_model(model, criterion, optimizer, train_loader, num_epochs, x_valid, y_valid):\n    pbar = tqdm(range(num_epochs))\n    for epoch in pbar:\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            \n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * inputs.size(0)\n            \n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n        \n        train_accuracy = correct / total * 100\n        \n        model.eval()\n        with torch.no_grad():\n            x_valid = x_valid.to(device)\n            preds = torch.argmax(model(x_valid), dim=1)\n            score = comp_score(y_valid, (preds + 1).cpu())\n            \n        epoch_loss = running_loss / len(train_loader.dataset)\n        pbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f} => QWK score: {score:.4f} => Accuracy: {train_accuracy:.2f}%\")\n    return model \n\n# Note: Make sure to define the 'comp_score' function and ensure 'device' is properly set (e.g., 'cuda' or 'cpu').","metadata":{"execution":{"iopub.status.busy":"2024-06-05T06:24:28.556069Z","iopub.execute_input":"2024-06-05T06:24:28.556430Z","iopub.status.idle":"2024-06-05T06:24:28.572736Z","shell.execute_reply.started":"2024-06-05T06:24:28.556401Z","shell.execute_reply":"2024-06-05T06:24:28.571805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# Check if CUDA is available and set device accordingly\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Hyperparameters\ninput_size = 1024       # Input vector dimensionality\nhidden_size1 = 3200     # Size of the first hidden layer\nhidden_size2 = 3200\nhidden_size3 = 1600 # Size of the second hidden layer\noutput_size = 6         # Number of output classes\nlearning_rate = 0.01   # Learning rate\nnum_epochs = 300         # Number of training epochs\nbatch_size = 128        # Batch size\n\nindices = np.arange(len(all_train_embeds))\ntrain_indices, valid_indices = train_test_split(indices, test_size=0.1, random_state=1000)\n\nX_train = all_train_embeds[train_indices]\ny_train = train.loc[train_indices, 'score'].values\nX_valid = all_train_embeds[valid_indices]\ny_valid = train.loc[valid_indices, 'score'].values\n\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train - 1, dtype=torch.long) \nX_valid_tensor = torch.tensor(X_valid, dtype=torch.float32)\ny_valid_tensor = torch.tensor(y_valid, dtype=torch.long)\n\ntrain_data = TensorDataset(X_train_tensor, y_train_tensor)\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-05T06:42:07.088756Z","iopub.execute_input":"2024-06-05T06:42:07.089101Z","iopub.status.idle":"2024-06-05T06:42:07.147646Z","shell.execute_reply.started":"2024-06-05T06:42:07.089073Z","shell.execute_reply":"2024-06-05T06:42:07.146654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hidden_sizes = [3200, 3200, 1600, 800, 128] \nmodel = AESModel(input_size, hidden_sizes) \nmodel.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\nmodel = train_model(model, criterion, optimizer, train_loader, num_epochs, X_valid_tensor, y_valid_tensor)","metadata":{"execution":{"iopub.status.busy":"2024-06-05T06:42:09.774388Z","iopub.execute_input":"2024-06-05T06:42:09.774715Z","iopub.status.idle":"2024-06-05T06:48:49.370783Z","shell.execute_reply.started":"2024-06-05T06:42:09.774691Z","shell.execute_reply":"2024-06-05T06:48:49.369912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_test_embeds = np.concatenate([test_sentence_embeddings], axis=1)\nall_test_embeds_tensor = torch.tensor(all_test_embeds, dtype=torch.float32)\nall_test_embeds_tensor = all_test_embeds_tensor.to(device)\n\nall_preds = []\n\nmodel.eval()\nwith torch.no_grad():\n    test_preds = model(all_test_embeds_tensor).argmax(dim=1).cpu().numpy()\nall_preds.append(test_preds + 1) \n\nsub = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv\")\n\nsub[\"score\"] = all_preds[0]\n\nsub.score = sub.score.astype('int32')\n\nsub.to_csv(\"/kaggle/working/submission.csv\", index=False)\n\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-05T06:51:34.269964Z","iopub.execute_input":"2024-06-05T06:51:34.270524Z","iopub.status.idle":"2024-06-05T06:51:34.291727Z","shell.execute_reply.started":"2024-06-05T06:51:34.270491Z","shell.execute_reply":"2024-06-05T06:51:34.290836Z"},"trusted":true},"execution_count":null,"outputs":[]}]}